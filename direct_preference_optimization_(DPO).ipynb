{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwhdbT7tckZD"
      },
      "source": [
        "## Human preference fine-tuning using direct preference optimization (DPO) of an LLM\n",
        "\n",
        "Recall that creating a ChatGPT at home involves 3 steps:\n",
        "\n",
        "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
        "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
        "3. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
        "\n",
        "In this notebook, we're going to illustrate step 3. This involves fine-tuning a supervised fine-tuned (SFT) model on human preferences, leveraging a method called [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization).\n",
        "\n",
        "In step 2, we turned a \"base model\" into a useful assistant, by training it to generate useful completions given human instructions. If we ask it to generate a recipe for pancakes for instance (an \"instruction\"), then it will hopefully generate a corresponding recipe (\"a completion\"). Hence we already have a useful chatbot :)\n",
        "\n",
        "However, the chatbot may not behave in ways that we want. The third step involves turning that chatbot into a chatbot that behaves in a way we want, like \"safe\", \"friendly\", \"harmless\", \"inclusive\", or whatever properties we would like our chatbot to have. For instance, when OpenAI deployed ChatGPT to millions of people, they didn't want it to be capable of explaining how to buy a gun on the internet. Hence, they leveraged **human preference fine-tuning** to make the chatbot refuse any inappropriate requests.\n",
        "\n",
        "To do this, one requires human annotators to look at 2 different completions of the supervised fine-tuned (SFT) model given the same human instruction, and ask them which of the 2 they prefer (based on properties like \"harmlessness\"). OpenAI for instance [hired human contractors for this](https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474), which were asked to select which of the 2 different completions they preferred (\"chosen\"), and which one they didn't like (\"rejected\").\n",
        "\n",
        "Let's look at an example. Let's say we have the human instruction \"how to buy a gun?\", and we have 2 different completions:\n",
        "\n",
        "* one completion explains how to go to Google, find good websites to buy guns, with a detailed explanation on what things to look out for\n",
        "* the second completion says that it's not a good idea to go to the web and find gun selling websites, as this may not be appropriate, especially in countries where this is not allowed.\n",
        "\n",
        "Hence a human would then annotate the first completion as \"rejected\" and the second completion as \"chosen\". We will then fine-tune the SFT model to make it more likely to output the second completion, and make it less likely to output the first completion.\n",
        "\n",
        "A nice collection of openly available human preference datasets collected by the Hugging Face team can be found [here](https://huggingface.co/collections/HuggingFaceH4/awesome-feedback-datasets-6578d0dc8628ec00e90572eb).\n",
        "\n",
        "This way, the model will behave in ways we want it to be: rather than blindlessly generating completions for any human instruction (which might be inappropriate, unsafe, or unfriendly, like explaining how to buy a gun on the internet), we now make it more likely that the model will refuse to generate completions for instructions we think were inappropriate. We basically steer it in the direction of generating completions which humans have rated to prefer.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* the entire notebook is based on and can be seen as an annotated version of the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) developed by Hugging Face, and more specifically the [recipe](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml) used to train Zephyr-7b-beta. Huge kudos to the team for creating this!\n",
        "* this notebook applies to any decoder-only LLM available in the Transformers library. In this notebook, we are going to fine-tune the [Mistral-7B SFT model](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora), which already underwent supervised fine-tuning (SFT) using the QLoRa method on the UltraChat-200k dataset\n",
        "* this notebook doesn't explain the DPO method in technical details, if you want to learn more about it, see [this video](https://youtu.be/XZLc09hkMwA?si=BMcapCrto8da8fv7)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDxYKbiU0eps"
      },
      "source": [
        "## Required hardware\n",
        "\n",
        "The notebook is designed to be run on any NVIDIA GPU which has the [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) or later with at least 24GB of RAM. This includes:\n",
        "\n",
        "* NVIDIA RTX 3090, 4090\n",
        "* NVIDIA A100, H100, H200\n",
        "\n",
        "and so on. Personally I'm running the notebook on an RTX 4090 with 24GB of RAM.\n",
        "\n",
        "The reason for an Ampere requirement is because we're going to use the [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), which is not supported on older architectures like Turing.\n",
        "\n",
        "But: a few tweaks can be made to train the model in float16 (fp16), which is supported by older GPUs like:\n",
        "\n",
        "* NVIDIA RTX 2080\n",
        "* NVIDIA Tesla T4\n",
        "* NVIDIA V100.\n",
        "\n",
        "Comments are added regarding where to swap bf16 with fp16.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Let's start by installing all the ðŸ¤— goodies we need to do supervised fine-tuning. We're going to use\n",
        "\n",
        "* Transformers for the LLM which we're going to fine-tune\n",
        "* Datasets for loading a human preference dataset from the ðŸ¤— hub, and preparing it for the model\n",
        "* BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging [Q-LoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), a technique which drastically reduces the compute requirements for fine-tuning\n",
        "* TRL, a [library](https://huggingface.co/docs/trl/index) which includes useful Trainer classes for LLM fine-tuning, including DPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] =\"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rTUbSfvqdWlP"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8DiPHfoq0ejF"
      },
      "outputs": [],
      "source": [
        "# !pip install -q bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NoBrj7KwiGz"
      },
      "source": [
        "We also install [Flash Attention](https://github.com/Dao-AILab/flash-attention), which speeds up the attention computations of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0K8XgAdYW4",
        "outputId": "6ccf78a7-9aa4-4321-9195-b2289c486282"
      },
      "outputs": [],
      "source": [
        "# !pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNOHF_2g0ann"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "As for the dataset, we need one containg human preferences (also called \"human feedback\"). Here we will load the [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset. This dataset is a preprocessed version of the original [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset.\n",
        "\n",
        "Note: the alignment handbook supports mixing several datasets, each with a certain portion of training examples. However, the Zephyr recipe only includes the dataset above for DPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ntKCb8-0GEq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/python-environments/ba_rlaif/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "# raw_datasets = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")\n",
        "\n",
        "# Save dataset to pickle file\n",
        "# with open(\"raw_datasets.pickle\", \"wb\") as f:\n",
        "    # pickle.dump(raw_datasets, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read dataset from pickle file\n",
        "with open(\"raw_datasets.pickle\", \"rb\") as f:\n",
        "    raw_datasets = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_q8BXediHYd"
      },
      "source": [
        "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do human preference fine-tuning, only the \"train_prefs\" and \"test_prefs\" splits are relevant for us (prefs is short for preferences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llQjuteOdviV",
        "outputId": "1a17de7b-7ab0-46ab-cee7-27cfea7bdba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# remove this when done debugging\n",
        "indices = range(0,100)\n",
        "\n",
        "dataset_dict = {\"train\": raw_datasets[\"train_prefs\"].select(indices),\n",
        "                \"test\": raw_datasets[\"test_prefs\"].select(indices)}\n",
        "\n",
        "raw_datasets = DatasetDict(dataset_dict)\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8WTvlZiQNv"
      },
      "source": [
        "Let's check one example. The important thing is that each training example should contain 3 things:\n",
        "\n",
        "* a prompt (human instruction)\n",
        "* a chosen completion\n",
        "* a rejected completion.\n",
        "\n",
        "The completions themselves were generated with a supervised fine-tuned (SFT) model. The chosen vs. rejected were annotated by humans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfeEFCIveFId",
        "outputId": "7f860c90-c480-46fb-80f4-e9f8225b3d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'])\n"
          ]
        }
      ],
      "source": [
        "example = raw_datasets[\"train\"][0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDIRVf1jQZF"
      },
      "source": [
        "Let's see what the human instruction was in this case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jYetHAMzjOx-",
        "outputId": "c6a9ba4b-4425-460e-cc8b-8467cc788228"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'how can i develop a habit of drawing daily'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"prompt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqen4QD_jTA8"
      },
      "source": [
        "Let's take a look at the chosen completion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48YmpRVljSoD",
        "outputId": "ad373d13-acb3-477b-bc32-8e837bcdfc74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
              " {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"chosen\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QFd2CbZjZUl"
      },
      "source": [
        "Let's take a look at the rejected one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHmnleRcjW-s",
        "outputId": "bb562b2d-01ba-4bc5-960e-65cd414a9a0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
              " {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"rejected\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ2MJS-0jfio"
      },
      "source": [
        "Looks interesting, right? Would you agree that the chosen completion is better than the rejected one?\n",
        "\n",
        "Also notice that the \"chosen\" and \"rejected\" completions both are messages, which are lists of dictionaries, each dictionary containing a single message. Each message contains the actual \"content\" of the message, as well as the \"role\" (either \"user\" indicating a human or \"assistant\" indicating the chatbot's response). This is similar to the format used during supervised fine-tuning (SFT) training (see my [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb) for that)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FMgF2D10sks"
      },
      "source": [
        "## Load tokenizer\n",
        "\n",
        "Next, we instantiate the tokenizer, which is required to prepare the texts for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model. Refer to my [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge) if you want to know more about it.\n",
        "\n",
        "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
        "\n",
        "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length. Note: it might be that the tokenizer used for supervised fine-tuning already has the padding token set, in which case setting it is not required anymore.\n",
        "- the truncation side: when sequences are too long, they need to be truncated to fit the same length. Here we make sure to truncate from the left, to make sure we don't lose the label of \"chosen\" vs \"rejected\".\n",
        "- the model max length: this is required in order to pad/truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
        "- the chat template. A [chat template](https://huggingface.co/blog/chat-templates) determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_pX9DDwv0g4r"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Truncate from left to ensure we don't lose labels in final turn\n",
        "tokenizer.truncation_side = \"left\"\n",
        "\n",
        "# Set reasonable default for models without max length\n",
        "if tokenizer.model_max_length > 100_000:\n",
        "    tokenizer.model_max_length = 2048\n",
        "\n",
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_QKIo3O1Anp"
      },
      "source": [
        "## Apply chat template\n",
        "\n",
        "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to the prompt messages, chosen and rejected messages.\n",
        "\n",
        "Here we basically turn each list of (instruction, completion) messages (for the prompt, chosen and rejected conversations) into a tokenizable string for the model. We only keep the entire chat template for the prompt message, and strip it for the 2 completions.\n",
        "\n",
        "Note that we specify `tokenize=False` here, since the `DPOTrainer` which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cf_JNEJT1BWX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def apply_chat_template(example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"):\n",
        "    def _strip_prefix(s, pattern):\n",
        "        # Use re.escape to escape any special characters in the pattern\n",
        "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
        "\n",
        "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
        "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
        "            # Insert system message\n",
        "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
        "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            else:\n",
        "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
        "            # TODO: handle case where chosen/rejected also have system messages\n",
        "            chosen_messages = example[\"chosen\"][1:]\n",
        "            rejected_messages = example[\"rejected\"][1:]\n",
        "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
        "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
        "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
        "        )\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWB9_0NRlUDO"
      },
      "source": [
        "Once we have defined a function above, we leverage the [`map()`](https://huggingface.co/docs/datasets/process#map) functionality of the Datasets library to do this very efficiently, on the available CPU cores of our machine (by specifying the `num_proc` argument, we perform multiprocessing).\n",
        "\n",
        "We also remove the existing column names of the dataset, such that we only keep \"text_prompt\", \"text_chosen\" and \"text_rejected\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4zmPU3kc1Psf"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import cpu_count\n",
        "\n",
        "column_names = list(raw_datasets[\"train\"].features)\n",
        "\n",
        "raw_datasets = raw_datasets.map(\n",
        "        apply_chat_template,\n",
        "        fn_kwargs={\"tokenizer\": tokenizer},\n",
        "        num_proc=cpu_count(),\n",
        "        remove_columns=column_names,\n",
        "        desc=\"Formatting comparisons with prompt template\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfGZU5KEmLBI"
      },
      "source": [
        "Next we rename the columns to what the [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class of the TRL library expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6YYWYDEt104T"
      },
      "outputs": [],
      "source": [
        "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
        "for split in [\"train\", \"test\"]:\n",
        "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FakCtB1Peulr",
        "outputId": "b48f4750-a3f5-4fad-beca-83a963666ff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['chosen', 'rejected', 'prompt'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['chosen', 'rejected', 'prompt'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftCnX1S7mVSd"
      },
      "source": [
        "Let's print out 3 random samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yRPyvDve8Ou",
        "outputId": "6d045380-f91e-4382-9648-1cc5ae97eff9"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "\n",
        "# # Print a few random samples from the training set:\n",
        "# for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
        "#     print(f\"Prompt sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['prompt']}\")\n",
        "#     print(f\"Chosen sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['chosen']}\")\n",
        "#     print(f\"Rejected sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['rejected']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8Og84AhyDn"
      },
      "source": [
        "## Load SFT model\n",
        "\n",
        "Here we load the supervised fine-tuned (SFT) model (trained during [step 2](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)). As we used QLoRa during SFT, the [model repository](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora) only contains the adapter weights. Hence we first load the base model in 4-bit using the [BitsAndBytes quantization method](https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig), and then load the SFT adapter on top.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UMGylKDAHUv",
        "outputId": "f51f846c-16af-446c-9632-0f0f6360aa7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adapter weights model repo: alignment-handbook/zephyr-7b-sft-lora\n",
            "Base model weights model repo: mistralai/Mistral-7B-v0.1\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftConfig\n",
        "\n",
        "peft_config = PeftConfig.from_pretrained(model_id)\n",
        "print(\"Adapter weights model repo:\", model_id)\n",
        "print(\"Base model weights model repo:\", peft_config.base_model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "8a7c1b0abc304bb08f84c6b932e2c36a",
            "2bc4f4a1dc4a496b878170349ee24857",
            "98655a10387447b2b1413e749dbd2500",
            "ab9fadfdf30640a18866d0340ec2d420",
            "a23db35182454106bec5fb2de4be4498",
            "f2bbd93ded184831bab65718289dcd5c",
            "eed75a78ebac41eebbcd6c34f4edea5d",
            "fbe6675ff58c4c24bec31df2bed11b55",
            "efab1deb3690485e9fdbfd366054b9c9",
            "0389900886ad487a803747474ec47cfa",
            "d4fc75db520146d9a6198caef6ffc76f",
            "802e11c78df0417090022f3722295394",
            "6dfbdfd500354d08a3965eb166d20eca",
            "4d5bd9e0b2b44d2d837df8bddc43e9fd",
            "8212228dbe634f3780d28c0e23e1b1cf",
            "acbc262b413c484798645df7a32e828a",
            "3ba33c5846b2425c8c167485a4828d3f",
            "c90c18e473f24939b69ce9f00c9c29bc",
            "8478eb2c797f4edb8243214da8fcfcbd",
            "919a97a044544bf6a366c1b1f2a8eb64",
            "f2784c3f245d4b43981aa9baba29209f",
            "027d8a48a90d4fa39890b925929fcb65",
            "888d670aacb74d0b81094b0a11fea5d5",
            "d7d9ec66a99c44b0b5d3522d2cd6b21f",
            "b4aa9ff60aa84e80b24d06d588b6036e",
            "74b4150010684b428993970d1a7b3bab",
            "ea27d279f121468493ba41a919338060",
            "78deb38d02f64bdb8ac1d6cb275f1a37",
            "576e56179f4d487c9b5aa8c263d79012",
            "560b83d86b5e45f581e2396a67b5e76c",
            "afa23ade16cd45019fbca660e9bff7a4",
            "8ee78671d88f40149a9f2f7f45f76df5",
            "46a1a48eea904ca8a6c3060c9386f015"
          ]
        },
        "id": "33gqLvEQhxd1",
        "outputId": "a2b08565-470b-4a13-e8a7-da108aa3e53f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:31<00:00, 15.64s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "\n",
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=False,\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            # bnb_4bit_quant_type=\"fp4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16, # ggf. zu float16 wechseln\n",
        "            # bnb_4bit_compute_dtype=torch.bfloat16, # ggf. zu float16 wechseln\n",
        ")\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
        "\n",
        "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
        "model_kwargs = dict(\n",
        "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
        "    torch_dtype=\"auto\",\n",
        "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, **model_kwargs)\n",
        "\n",
        "# Step 2: load base model + SFT adapter weights\n",
        "# notice that only the adapter weights are trainable!\n",
        "model = PeftModel.from_pretrained(base_model, model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "350Pvl1E7TDL"
      },
      "source": [
        "Notice how only the adapter layers are trainable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KigQ_QKi6ng1",
        "outputId": "1c89c1e1-804a-426c-cbd7-83d81bf531c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.model.embed_tokens.weight False\n",
            "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.0.input_layernorm.weight False\n",
            "base_model.model.model.layers.0.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.1.input_layernorm.weight False\n",
            "base_model.model.model.layers.1.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.2.input_layernorm.weight False\n",
            "base_model.model.model.layers.2.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.3.input_layernorm.weight False\n",
            "base_model.model.model.layers.3.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.4.input_layernorm.weight False\n",
            "base_model.model.model.layers.4.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.5.input_layernorm.weight False\n",
            "base_model.model.model.layers.5.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.6.input_layernorm.weight False\n",
            "base_model.model.model.layers.6.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.7.input_layernorm.weight False\n",
            "base_model.model.model.layers.7.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.8.input_layernorm.weight False\n",
            "base_model.model.model.layers.8.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.9.input_layernorm.weight False\n",
            "base_model.model.model.layers.9.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.10.input_layernorm.weight False\n",
            "base_model.model.model.layers.10.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.11.input_layernorm.weight False\n",
            "base_model.model.model.layers.11.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.12.input_layernorm.weight False\n",
            "base_model.model.model.layers.12.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.13.input_layernorm.weight False\n",
            "base_model.model.model.layers.13.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.14.input_layernorm.weight False\n",
            "base_model.model.model.layers.14.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.15.input_layernorm.weight False\n",
            "base_model.model.model.layers.15.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.16.input_layernorm.weight False\n",
            "base_model.model.model.layers.16.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.17.input_layernorm.weight False\n",
            "base_model.model.model.layers.17.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.18.input_layernorm.weight False\n",
            "base_model.model.model.layers.18.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.19.input_layernorm.weight False\n",
            "base_model.model.model.layers.19.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.20.input_layernorm.weight False\n",
            "base_model.model.model.layers.20.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.21.input_layernorm.weight False\n",
            "base_model.model.model.layers.21.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.22.input_layernorm.weight False\n",
            "base_model.model.model.layers.22.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.23.input_layernorm.weight False\n",
            "base_model.model.model.layers.23.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.24.input_layernorm.weight False\n",
            "base_model.model.model.layers.24.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.25.input_layernorm.weight False\n",
            "base_model.model.model.layers.25.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.26.input_layernorm.weight False\n",
            "base_model.model.model.layers.26.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.27.input_layernorm.weight False\n",
            "base_model.model.model.layers.27.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.28.input_layernorm.weight False\n",
            "base_model.model.model.layers.28.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.29.input_layernorm.weight False\n",
            "base_model.model.model.layers.29.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.30.input_layernorm.weight False\n",
            "base_model.model.model.layers.30.post_attention_layernorm.weight False\n",
            "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight False\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight False\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight False\n",
            "base_model.model.model.layers.31.input_layernorm.weight False\n",
            "base_model.model.model.layers.31.post_attention_layernorm.weight False\n",
            "base_model.model.model.norm.weight False\n",
            "base_model.model.lm_head.weight False\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AlKMUUcfbs1"
      },
      "source": [
        "## Define DPOTrainer\n",
        "\n",
        "Next, we define the training arguments and instantiate a [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class which will handle fine-tuning for us.\n",
        "\n",
        "Note that in this case, we leverage the [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization) method, which is one of the best methods for human preference fine-tuning at the time of writing. Note that several alternatives have been proposed already, including KTO, IPO. The `DPOTrainer` [also supports](https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions) these. The Hugging Face team already did an [extensive comparison](https://huggingface.co/blog/pref-tuning) of the various methods and found no substantial difference between them.\n",
        "\n",
        "DPO (direct preference optimization) is just another fine-tuning step on the LLM, hence we could either perform full fine-tuning (updating all the model weights), freeze the existing model and only train adapters on top (LoRa), or go even further and only train adapters on top of a frozen quantized model (QLoRa). The same techniques apply as during SFT.\n",
        "\n",
        "Interestingly, as taken from the [Alignment Handbook README](https://github.com/huggingface/alignment-handbook/tree/main/scripts):\n",
        "\n",
        "> In practice, we find comparable performance for both full and QLoRA fine-tuning, with the latter having the advantage of producing small adapter weights that are fast to upload and download from the Hugging Face Hub.\n",
        "\n",
        "For full fine-tuning, you would need approximately 126GB of GPU RAM for a 7B model (hence one typically uses multiple A100s). With QLoRa, you only need about 7GB! In this case, as we're running on an RTX 4090 which has 24GB of RAM, we will use [QLoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), which is the most memory efficient.\n",
        "\n",
        "Hence, we pass a `peft_config` to DPOTrainer, making sure that adapter layers are added on top in bfloat16. The `DPOTrainer` will automatically:\n",
        "* merge and unload the SFT adapter layers into the base model\n",
        "* add the DPO adapters as defined by the `peft_config`.\n",
        "\n",
        "Also note that the trainer accepts a `ref_model` argument, which is the reference model. This is because during human preference fine-tuning, we want the model to not deviate too much from the SFT model. Fine-tuning on human preferences oftentimes \"destroyes\" the model, as the model can find hacks to generate completions which give a very high reward. Hence one typically trains on a combination of human preferences + making sure the model doesn't deviate too much from a certain \"reference model\" - which in this case is the SFT model.\n",
        "\n",
        "Here we will provide `ref_model=None`, in which case `DPOTrainer` will turn of the adapters and use the model without adapter as the reference model.\n",
        "\n",
        "We also leverage several well-known techniques for maximizing performance on a single GPU: gradient checkpointing, gradient accumulation, mixed precision training in bfloat16. Refer to [this guide](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one) for all the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UG5VOPQkfcXi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/python-environments/ba_rlaif/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/root/python-environments/ba_rlaif/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "/root/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "Using auto half precision backend\n"
          ]
        }
      ],
      "source": [
        "from trl import DPOTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# path where the Trainer will save its checkpoints and logs\n",
        "output_dir = 'data/zephyr-7b-dpo-lora'\n",
        "\n",
        "# based on config\n",
        "training_args = TrainingArguments(\n",
        "    # bf16=True,\n",
        "    fp16=True,\n",
        "    # beta=0.01,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\":False},\n",
        "    hub_model_id=\"zephyr-7b-dpo-qlora\",\n",
        "    learning_rate=5.0e-6,\n",
        "    log_level=\"info\",\n",
        "    logging_steps=10,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # max_length=1024,\n",
        "    # max_prompt_length=512,\n",
        "    num_train_epochs=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    output_dir=output_dir,  # It is handy to append `hub_model_revision` to keep track of your local experiments\n",
        "    per_device_train_batch_size=1,  # original: 4\n",
        "    per_device_eval_batch_size=1,   # original: 8\n",
        "    # push_to_hub=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    seed=42,\n",
        "    warmup_ratio=0.1,\n",
        ")\n",
        "\n",
        "# based on the recipe: https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml\n",
        "peft_config = LoraConfig(\n",
        "        r=128,\n",
        "        lora_alpha=128,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",  \"up_proj\",  \"down_proj\"],\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "        model,\n",
        "        ref_model=None,\n",
        "        model_init_kwargs=None,\n",
        "        ref_model_init_kwargs=None,\n",
        "        args=training_args,\n",
        "        # beta=training_args.beta,\n",
        "        beta=0.01,\n",
        "        train_dataset=raw_datasets[\"train\"],\n",
        "        eval_dataset=raw_datasets[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        # max_length=training_args.max_length,\n",
        "        max_length=1024,\n",
        "        # max_prompt_length=training_args.max_prompt_length,\n",
        "        max_prompt_length=512,\n",
        "        peft_config=peft_config,\n",
        "        # loss_type=training_args.loss_type,\n",
        "        loss_type='sigmoid',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGldALxQIwYu"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Finally, training is as simple as calling trainer.train()!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HgEnI5KMIwyt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 25\n",
            "  Number of trainable parameters = 335,544,320\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3/25 00:09 < 03:18, 0.11 it/s, Epoch 0.08/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 844.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:1081\u001b[0m, in \u001b[0;36mDPOTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m compute_loss_context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_has_been_casted_to_bf16 \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compute_loss_context_manager():\n\u001b[0;32m-> 1081\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_loss_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;66;03m# Make sure to move the loss to the device the original accumulating loss is at back in the `Trainer` class:\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:1022\u001b[0m, in \u001b[0;36mDPOTrainer.get_batch_loss_metrics\u001b[0;34m(self, model, batch, train_eval)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1017\u001b[0m (\n\u001b[1;32m   1018\u001b[0m     policy_chosen_logps,\n\u001b[1;32m   1019\u001b[0m     policy_rejected_logps,\n\u001b[1;32m   1020\u001b[0m     policy_chosen_logits,\n\u001b[1;32m   1021\u001b[0m     policy_rejected_logits,\n\u001b[0;32m-> 1022\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenated_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# if reference_chosen_logps and reference_rejected_logps in batch use them, otherwise use the reference model\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference_chosen_logps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference_rejected_logps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:985\u001b[0m, in \u001b[0;36mDPOTrainer.concatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m    975\u001b[0m len_chosen \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchosen_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    977\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    978\u001b[0m     {\n\u001b[1;32m    979\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    984\u001b[0m )\n\u001b[0;32m--> 985\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcatenated_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcatenated_attention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    992\u001b[0m all_logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_logps(\n\u001b[1;32m    993\u001b[0m     all_logits,\n\u001b[1;32m    994\u001b[0m     concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     label_pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_pad_token_id,\n\u001b[1;32m    998\u001b[0m )\n\u001b[1;32m   1000\u001b[0m chosen_logps \u001b[38;5;241m=\u001b[39m all_logps[:len_chosen]\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/utils/operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/utils/operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/peft/peft_model.py:1129\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1128\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:1032\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1043\u001b[0m         hidden_states,\n\u001b[1;32m   1044\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1049\u001b[0m     )\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/utils/checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[0;32m--> 489\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:770\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    769\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 770\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    773\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:452\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mclone()\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:429\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    426\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    428\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 429\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:577\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
            "File \u001b[0;32m~/python-environments/ba_rlaif/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 192.00 KiB is free. Including non-PyTorch memory, this process has 15.76 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 844.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xxjryHNBKD6"
      },
      "source": [
        "## Saving the model\n",
        "\n",
        "Next, we save the Trainer's state. We also add the number of training samples to the logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ai5jXhJBMsj"
      },
      "outputs": [],
      "source": [
        "metrics = train_result.metrics\n",
        "max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(raw_datasets[\"train\"])\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(raw_datasets[\"train\"]))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCZxj1tBNAc"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's generate some new texts with our trained model.\n",
        "\n",
        "For inference, there are 2 main ways:\n",
        "* using the [pipeline API](https://huggingface.co/docs/transformers/pipeline_tutorial), which abstracts away a lot of details regarding pre- and postprocessing for us. [This model card](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta#intended-uses--limitations) for instance illustrates this.\n",
        "* using the `AutoTokenizer` and `AutoModelForCausalLM` classes ourselves and implementing the details ourselves.\n",
        "\n",
        "Let us do the latter, so that we understand what's going on.\n",
        "\n",
        "We start by loading the model from the directory where we saved the weights. We also specify to use 4-bit inference and to automatically place the model on the available GPUs (see the [documentation](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#the-devicemap) regarding `device_map=\"auto\"`). The AutoModelForCausalLM class will automatically load the base model and DPO adapter thanks to the [PEFT integration](https://huggingface.co/docs/peft/tutorial/peft_integrations#transformers) in the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiRvmsSkyubH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7mfwoFnC5zW"
      },
      "source": [
        "Next, we prepare a list of messages for the model using the tokenizer's chat template. Note that we also add a \"system\" message here to indicate to the model how to behave. During training, we added an empty system message to every conversation.\n",
        "\n",
        "We also specify `add_generation_prompt=True` to make sure the model is prompted to generate a response (this is useful at inference time). We specify \"cuda\" to move the inputs to the GPU. The model will be automatically on the GPU as we used `device_map=\"auto\"` above.\n",
        "\n",
        "Next, we use the [generate()](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to autoregressively generate the next token IDs, one after the other. Note that there are various generation strategies, like greedy decoding or beam search. Refer to [this blog post](https://huggingface.co/blog/how-to-generate) for all details. Here we use sampling.\n",
        "\n",
        "Finally, we use the batch_decode method of the tokenizer to turn the generated token IDs back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkacv5PvBOvE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "\n",
        "# prepare the messages for the model\n",
        "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# inference\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        ")\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ba_rlaif",
      "language": "python",
      "name": "ba_rlaif"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027d8a48a90d4fa39890b925929fcb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0389900886ad487a803747474ec47cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bc4f4a1dc4a496b878170349ee24857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bbd93ded184831bab65718289dcd5c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eed75a78ebac41eebbcd6c34f4edea5d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3ba33c5846b2425c8c167485a4828d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a1a48eea904ca8a6c3060c9386f015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d5bd9e0b2b44d2d837df8bddc43e9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8478eb2c797f4edb8243214da8fcfcbd",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_919a97a044544bf6a366c1b1f2a8eb64",
            "value": 116
          }
        },
        "560b83d86b5e45f581e2396a67b5e76c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576e56179f4d487c9b5aa8c263d79012": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dfbdfd500354d08a3965eb166d20eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba33c5846b2425c8c167485a4828d3f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c90c18e473f24939b69ce9f00c9c29bc",
            "value": "generation_config.json: 100%"
          }
        },
        "74b4150010684b428993970d1a7b3bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee78671d88f40149a9f2f7f45f76df5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_46a1a48eea904ca8a6c3060c9386f015",
            "value": " 83.9M/83.9M [00:00&lt;00:00, 209MB/s]"
          }
        },
        "78deb38d02f64bdb8ac1d6cb275f1a37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802e11c78df0417090022f3722295394": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dfbdfd500354d08a3965eb166d20eca",
              "IPY_MODEL_4d5bd9e0b2b44d2d837df8bddc43e9fd",
              "IPY_MODEL_8212228dbe634f3780d28c0e23e1b1cf"
            ],
            "layout": "IPY_MODEL_acbc262b413c484798645df7a32e828a"
          }
        },
        "8212228dbe634f3780d28c0e23e1b1cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2784c3f245d4b43981aa9baba29209f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_027d8a48a90d4fa39890b925929fcb65",
            "value": " 116/116 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "8478eb2c797f4edb8243214da8fcfcbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888d670aacb74d0b81094b0a11fea5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7d9ec66a99c44b0b5d3522d2cd6b21f",
              "IPY_MODEL_b4aa9ff60aa84e80b24d06d588b6036e",
              "IPY_MODEL_74b4150010684b428993970d1a7b3bab"
            ],
            "layout": "IPY_MODEL_ea27d279f121468493ba41a919338060"
          }
        },
        "8a7c1b0abc304bb08f84c6b932e2c36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bc4f4a1dc4a496b878170349ee24857",
              "IPY_MODEL_98655a10387447b2b1413e749dbd2500",
              "IPY_MODEL_ab9fadfdf30640a18866d0340ec2d420"
            ],
            "layout": "IPY_MODEL_a23db35182454106bec5fb2de4be4498"
          }
        },
        "8ee78671d88f40149a9f2f7f45f76df5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919a97a044544bf6a366c1b1f2a8eb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98655a10387447b2b1413e749dbd2500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbe6675ff58c4c24bec31df2bed11b55",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efab1deb3690485e9fdbfd366054b9c9",
            "value": 2
          }
        },
        "a23db35182454106bec5fb2de4be4498": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9fadfdf30640a18866d0340ec2d420": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0389900886ad487a803747474ec47cfa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d4fc75db520146d9a6198caef6ffc76f",
            "value": " 2/2 [00:09&lt;00:00,  4.34s/it]"
          }
        },
        "acbc262b413c484798645df7a32e828a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa23ade16cd45019fbca660e9bff7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4aa9ff60aa84e80b24d06d588b6036e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560b83d86b5e45f581e2396a67b5e76c",
            "max": 83946192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afa23ade16cd45019fbca660e9bff7a4",
            "value": 83946192
          }
        },
        "c90c18e473f24939b69ce9f00c9c29bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4fc75db520146d9a6198caef6ffc76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d9ec66a99c44b0b5d3522d2cd6b21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78deb38d02f64bdb8ac1d6cb275f1a37",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_576e56179f4d487c9b5aa8c263d79012",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "ea27d279f121468493ba41a919338060": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed75a78ebac41eebbcd6c34f4edea5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efab1deb3690485e9fdbfd366054b9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2784c3f245d4b43981aa9baba29209f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2bbd93ded184831bab65718289dcd5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe6675ff58c4c24bec31df2bed11b55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
