{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwhdbT7tckZD"
      },
      "source": [
        "## Human preference fine-tuning using direct preference optimization (DPO) of an LLM\n",
        "\n",
        "Recall that creating a ChatGPT at home involves 3 steps:\n",
        "\n",
        "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
        "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
        "3. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
        "\n",
        "In this notebook, we're going to illustrate step 3. This involves fine-tuning a supervised fine-tuned (SFT) model on human preferences, leveraging a method called [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization).\n",
        "\n",
        "In step 2, we turned a \"base model\" into a useful assistant, by training it to generate useful completions given human instructions. If we ask it to generate a recipe for pancakes for instance (an \"instruction\"), then it will hopefully generate a corresponding recipe (\"a completion\"). Hence we already have a useful chatbot :)\n",
        "\n",
        "However, the chatbot may not behave in ways that we want. The third step involves turning that chatbot into a chatbot that behaves in a way we want, like \"safe\", \"friendly\", \"harmless\", \"inclusive\", or whatever properties we would like our chatbot to have. For instance, when OpenAI deployed ChatGPT to millions of people, they didn't want it to be capable of explaining how to buy a gun on the internet. Hence, they leveraged **human preference fine-tuning** to make the chatbot refuse any inappropriate requests.\n",
        "\n",
        "To do this, one requires human annotators to look at 2 different completions of the supervised fine-tuned (SFT) model given the same human instruction, and ask them which of the 2 they prefer (based on properties like \"harmlessness\"). OpenAI for instance [hired human contractors for this](https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474), which were asked to select which of the 2 different completions they preferred (\"chosen\"), and which one they didn't like (\"rejected\").\n",
        "\n",
        "Let's look at an example. Let's say we have the human instruction \"how to buy a gun?\", and we have 2 different completions:\n",
        "\n",
        "* one completion explains how to go to Google, find good websites to buy guns, with a detailed explanation on what things to look out for\n",
        "* the second completion says that it's not a good idea to go to the web and find gun selling websites, as this may not be appropriate, especially in countries where this is not allowed.\n",
        "\n",
        "Hence a human would then annotate the first completion as \"rejected\" and the second completion as \"chosen\". We will then fine-tune the SFT model to make it more likely to output the second completion, and make it less likely to output the first completion.\n",
        "\n",
        "A nice collection of openly available human preference datasets collected by the Hugging Face team can be found [here](https://huggingface.co/collections/HuggingFaceH4/awesome-feedback-datasets-6578d0dc8628ec00e90572eb).\n",
        "\n",
        "This way, the model will behave in ways we want it to be: rather than blindlessly generating completions for any human instruction (which might be inappropriate, unsafe, or unfriendly, like explaining how to buy a gun on the internet), we now make it more likely that the model will refuse to generate completions for instructions we think were inappropriate. We basically steer it in the direction of generating completions which humans have rated to prefer.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* the entire notebook is based on and can be seen as an annotated version of the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) developed by Hugging Face, and more specifically the [recipe](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml) used to train Zephyr-7b-beta. Huge kudos to the team for creating this!\n",
        "* this notebook applies to any decoder-only LLM available in the Transformers library. In this notebook, we are going to fine-tune the [Mistral-7B SFT model](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora), which already underwent supervised fine-tuning (SFT) using the QLoRa method on the UltraChat-200k dataset\n",
        "* this notebook doesn't explain the DPO method in technical details, if you want to learn more about it, see [this video](https://youtu.be/XZLc09hkMwA?si=BMcapCrto8da8fv7)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDxYKbiU0eps"
      },
      "source": [
        "## Required hardware\n",
        "\n",
        "The notebook is designed to be run on any NVIDIA GPU which has the [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) or later with at least 24GB of RAM. This includes:\n",
        "\n",
        "* NVIDIA RTX 3090, 4090\n",
        "* NVIDIA A100, H100, H200\n",
        "\n",
        "and so on. Personally I'm running the notebook on an RTX 4090 with 24GB of RAM.\n",
        "\n",
        "The reason for an Ampere requirement is because we're going to use the [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), which is not supported on older architectures like Turing.\n",
        "\n",
        "But: a few tweaks can be made to train the model in float16 (fp16), which is supported by older GPUs like:\n",
        "\n",
        "* NVIDIA RTX 2080\n",
        "* NVIDIA Tesla T4\n",
        "* NVIDIA V100.\n",
        "\n",
        "Comments are added regarding where to swap bf16 with fp16.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Let's start by installing all the ðŸ¤— goodies we need to do supervised fine-tuning. We're going to use\n",
        "\n",
        "* Transformers for the LLM which we're going to fine-tune\n",
        "* Datasets for loading a human preference dataset from the ðŸ¤— hub, and preparing it for the model\n",
        "* BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging [Q-LoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), a technique which drastically reduces the compute requirements for fine-tuning\n",
        "* TRL, a [library](https://huggingface.co/docs/trl/index) which includes useful Trainer classes for LLM fine-tuning, including DPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# pref_data_labeled_sample = pd.read_feather('data/pref_data_labeled.feather').sample(n=60000, random_state=42)\n",
        "# pref_data_labeled_sample = pref_data_labeled_sample.reset_index(drop=True)\n",
        "\n",
        "# pref_data_labeled_sample.to_feather('data/pref_data_labeled-60k_sample.feather')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] =\"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rTUbSfvqdWlP"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8DiPHfoq0ejF"
      },
      "outputs": [],
      "source": [
        "# !pip install -q bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NoBrj7KwiGz"
      },
      "source": [
        "We also install [Flash Attention](https://github.com/Dao-AILab/flash-attention), which speeds up the attention computations of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y0K8XgAdYW4",
        "outputId": "6ccf78a7-9aa4-4321-9195-b2289c486282"
      },
      "outputs": [],
      "source": [
        "# !pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNOHF_2g0ann"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "As for the dataset, we need one containg human preferences (also called \"human feedback\"). Here we will load the [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset. This dataset is a preprocessed version of the original [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset.\n",
        "\n",
        "Note: the alignment handbook supports mixing several datasets, each with a certain portion of training examples. However, the Zephyr recipe only includes the dataset above for DPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_ntKCb8-0GEq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/python_environments/ba_rlaif/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from datasets import load_dataset\n",
        "\n",
        "# # Load dataset\n",
        "# raw_datasets = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")\n",
        "\n",
        "# # Save dataset to pickle file\n",
        "# with open(\"data/raw_datasets.pickle\", \"wb\") as f:\n",
        "#     pickle.dump(raw_datasets, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read dataset from pickle file\n",
        "with open(\"data/custom_pref_data_7b.pickle\", \"rb\") as f:\n",
        "    raw_datasets = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_q8BXediHYd"
      },
      "source": [
        "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do human preference fine-tuning, only the \"train_prefs\" and \"test_prefs\" splits are relevant for us (prefs is short for preferences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llQjuteOdviV",
        "outputId": "1a17de7b-7ab0-46ab-cee7-27cfea7bdba7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', '__index_level_0__'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', '__index_level_0__'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# remove this when done debugging\n",
        "indices = range(0,100)\n",
        "\n",
        "# dataset_dict = {\"train\": raw_datasets[\"train\"],\n",
        "#                 \"test\": raw_datasets[\"test\"]}\n",
        "dataset_dict = {\"train\": raw_datasets[\"train\"].select(indices),\n",
        "                \"test\": raw_datasets[\"test\"].select(indices)}\n",
        "\n",
        "raw_datasets = DatasetDict(dataset_dict)\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_8WTvlZiQNv"
      },
      "source": [
        "Let's check one example. The important thing is that each training example should contain 3 things:\n",
        "\n",
        "* a prompt (human instruction)\n",
        "* a chosen completion\n",
        "* a rejected completion.\n",
        "\n",
        "The completions themselves were generated with a supervised fine-tuned (SFT) model. The chosen vs. rejected were annotated by humans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfeEFCIveFId",
        "outputId": "7f860c90-c480-46fb-80f4-e9f8225b3d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', '__index_level_0__'])\n"
          ]
        }
      ],
      "source": [
        "example = raw_datasets[\"train\"][0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDIRVf1jQZF"
      },
      "source": [
        "Let's see what the human instruction was in this case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jYetHAMzjOx-",
        "outputId": "c6a9ba4b-4425-460e-cc8b-8467cc788228"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Summarize the following text: \\n  I need advice on what to do about a situation involving myself and an old friend from high school. Here\\'s what went down:\\n\\nMe and some friends went out to the bar in my hometown last weekend. I was relatively sober. Ran into a female friend from high school who I used to have quite the little crush on. Now, I hadn\\'t really seen or talked to her for ~2 years, but from social media I knew that she had a boyfriend. \\n\\nAnyways, we start talking and it\\'s very clear to me that she is more than a little inebriated. While I\\'m not the best interpreter of how drunk a girl is, it seemed to me that she still had a firm hold of all her faculties and was able to hold a solid conversation and she wasn\\'t stumbling around everywhere. Now, very soon in the time I had been talking to her, she was all over me - getting real close to me and touching and flirting. According to my friends that I was with, it was blatantly clear that she was into me.\\n\\nI was skeptical, because I knew she had a boyfriend and she was also drunk. However, she started talking about how shitty her relationship is and how she used to have a crush on me in high school and a bunch of not subtle things like that. She then dragged me out onto the d-floor to dance. She was really getting into it, and I think she wanted me to kiss her, but I refrained because she was drunk and I wasn\\'t. Eventually her friends (who were her ride home) came and told her that they were leaving, so she told me goodbye and hugged me. I told her to text me and she said that she would.\\n\\nShe still has not texted me. So how should I interpret this? Was she just drunk and lashing out at her boyfriend? Or was it a case of \"sober thoughts are drunk actions\" and she\\'s actually interested in me? Any advice is appreciated!'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"prompt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqen4QD_jTA8"
      },
      "source": [
        "Let's take a look at the chosen completion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48YmpRVljSoD",
        "outputId": "ad373d13-acb3-477b-bc32-8e837bcdfc74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'Summarize the following text: \\n  I need advice on what to do about a situation involving myself and an old friend from high school. Here\\'s what went down:\\n\\nMe and some friends went out to the bar in my hometown last weekend. I was relatively sober. Ran into a female friend from high school who I used to have quite the little crush on. Now, I hadn\\'t really seen or talked to her for ~2 years, but from social media I knew that she had a boyfriend. \\n\\nAnyways, we start talking and it\\'s very clear to me that she is more than a little inebriated. While I\\'m not the best interpreter of how drunk a girl is, it seemed to me that she still had a firm hold of all her faculties and was able to hold a solid conversation and she wasn\\'t stumbling around everywhere. Now, very soon in the time I had been talking to her, she was all over me - getting real close to me and touching and flirting. According to my friends that I was with, it was blatantly clear that she was into me.\\n\\nI was skeptical, because I knew she had a boyfriend and she was also drunk. However, she started talking about how shitty her relationship is and how she used to have a crush on me in high school and a bunch of not subtle things like that. She then dragged me out onto the d-floor to dance. She was really getting into it, and I think she wanted me to kiss her, but I refrained because she was drunk and I wasn\\'t. Eventually her friends (who were her ride home) came and told her that they were leaving, so she told me goodbye and hugged me. I told her to text me and she said that she would.\\n\\nShe still has not texted me. So how should I interpret this? Was she just drunk and lashing out at her boyfriend? Or was it a case of \"sober thoughts are drunk actions\" and she\\'s actually interested in me? Any advice is appreciated!',\n",
              "  'role': 'user'},\n",
              " {'content': \" Met up with a girl that I had a crush on in high school and we hit it off. Turns out she has a boyfriend and it was obvious that she was drunk in the time we had been talking. She still hasn't texted me and I'm not sure how to interpret this.\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"chosen\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QFd2CbZjZUl"
      },
      "source": [
        "Let's take a look at the rejected one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHmnleRcjW-s",
        "outputId": "bb562b2d-01ba-4bc5-960e-65cd414a9a0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'Summarize the following text: \\n  I need advice on what to do about a situation involving myself and an old friend from high school. Here\\'s what went down:\\n\\nMe and some friends went out to the bar in my hometown last weekend. I was relatively sober. Ran into a female friend from high school who I used to have quite the little crush on. Now, I hadn\\'t really seen or talked to her for ~2 years, but from social media I knew that she had a boyfriend. \\n\\nAnyways, we start talking and it\\'s very clear to me that she is more than a little inebriated. While I\\'m not the best interpreter of how drunk a girl is, it seemed to me that she still had a firm hold of all her faculties and was able to hold a solid conversation and she wasn\\'t stumbling around everywhere. Now, very soon in the time I had been talking to her, she was all over me - getting real close to me and touching and flirting. According to my friends that I was with, it was blatantly clear that she was into me.\\n\\nI was skeptical, because I knew she had a boyfriend and she was also drunk. However, she started talking about how shitty her relationship is and how she used to have a crush on me in high school and a bunch of not subtle things like that. She then dragged me out onto the d-floor to dance. She was really getting into it, and I think she wanted me to kiss her, but I refrained because she was drunk and I wasn\\'t. Eventually her friends (who were her ride home) came and told her that they were leaving, so she told me goodbye and hugged me. I told her to text me and she said that she would.\\n\\nShe still has not texted me. So how should I interpret this? Was she just drunk and lashing out at her boyfriend? Or was it a case of \"sober thoughts are drunk actions\" and she\\'s actually interested in me? Any advice is appreciated!',\n",
              "  'role': 'user'},\n",
              " {'content': \" Girl that I went to high school with was really drunk and hitting on me, even though she has a boyfriend. What's the deal?\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"rejected\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ2MJS-0jfio"
      },
      "source": [
        "Looks interesting, right? Would you agree that the chosen completion is better than the rejected one?\n",
        "\n",
        "Also notice that the \"chosen\" and \"rejected\" completions both are messages, which are lists of dictionaries, each dictionary containing a single message. Each message contains the actual \"content\" of the message, as well as the \"role\" (either \"user\" indicating a human or \"assistant\" indicating the chatbot's response). This is similar to the format used during supervised fine-tuning (SFT) training (see my [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb) for that)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FMgF2D10sks"
      },
      "source": [
        "## Load tokenizer\n",
        "\n",
        "Next, we instantiate the tokenizer, which is required to prepare the texts for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model. Refer to my [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge) if you want to know more about it.\n",
        "\n",
        "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
        "\n",
        "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length. Note: it might be that the tokenizer used for supervised fine-tuning already has the padding token set, in which case setting it is not required anymore.\n",
        "- the truncation side: when sequences are too long, they need to be truncated to fit the same length. Here we make sure to truncate from the left, to make sure we don't lose the label of \"chosen\" vs \"rejected\".\n",
        "- the model max length: this is required in order to pad/truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
        "- the chat template. A [chat template](https://huggingface.co/blog/chat-templates) determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_pX9DDwv0g4r"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Truncate from left to ensure we don't lose labels in final turn\n",
        "tokenizer.truncation_side = \"left\"\n",
        "\n",
        "# Set reasonable default for models without max length\n",
        "if tokenizer.model_max_length > 100_000:\n",
        "    tokenizer.model_max_length = 2048\n",
        "\n",
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_QKIo3O1Anp"
      },
      "source": [
        "## Apply chat template\n",
        "\n",
        "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to the prompt messages, chosen and rejected messages.\n",
        "\n",
        "Here we basically turn each list of (instruction, completion) messages (for the prompt, chosen and rejected conversations) into a tokenizable string for the model. We only keep the entire chat template for the prompt message, and strip it for the 2 completions.\n",
        "\n",
        "Note that we specify `tokenize=False` here, since the `DPOTrainer` which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Cf_JNEJT1BWX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def apply_chat_template(example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"):\n",
        "    def _strip_prefix(s, pattern):\n",
        "        # Use re.escape to escape any special characters in the pattern\n",
        "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
        "\n",
        "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
        "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
        "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
        "            # Insert system message\n",
        "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
        "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            else:\n",
        "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
        "            # TODO: handle case where chosen/rejected also have system messages\n",
        "            chosen_messages = example[\"chosen\"][1:]\n",
        "            rejected_messages = example[\"rejected\"][1:]\n",
        "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
        "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
        "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
        "        )\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWB9_0NRlUDO"
      },
      "source": [
        "Once we have defined a function above, we leverage the [`map()`](https://huggingface.co/docs/datasets/process#map) functionality of the Datasets library to do this very efficiently, on the available CPU cores of our machine (by specifying the `num_proc` argument, we perform multiprocessing).\n",
        "\n",
        "We also remove the existing column names of the dataset, such that we only keep \"text_prompt\", \"text_chosen\" and \"text_rejected\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4zmPU3kc1Psf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Formatting comparisons with prompt template (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  5.91 examples/s]\n",
            "Formatting comparisons with prompt template (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 17.62 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from multiprocessing import cpu_count\n",
        "\n",
        "column_names = list(raw_datasets[\"train\"].features)\n",
        "\n",
        "raw_datasets = raw_datasets.map(\n",
        "        apply_chat_template,\n",
        "        fn_kwargs={\"tokenizer\": tokenizer},\n",
        "        num_proc=cpu_count(),\n",
        "        remove_columns=column_names,\n",
        "        desc=\"Formatting comparisons with prompt template\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfGZU5KEmLBI"
      },
      "source": [
        "Next we rename the columns to what the [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class of the TRL library expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6YYWYDEt104T"
      },
      "outputs": [],
      "source": [
        "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
        "for split in [\"train\", \"test\"]:\n",
        "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FakCtB1Peulr",
        "outputId": "b48f4750-a3f5-4fad-beca-83a963666ff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['chosen', 'rejected', 'prompt'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['chosen', 'rejected', 'prompt'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftCnX1S7mVSd"
      },
      "source": [
        "Let's print out 3 random samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yRPyvDve8Ou",
        "outputId": "6d045380-f91e-4382-9648-1cc5ae97eff9"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "\n",
        "# # Print a few random samples from the training set:\n",
        "# for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
        "#     print(f\"Prompt sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['prompt']}\")\n",
        "#     print(f\"Chosen sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['chosen']}\")\n",
        "#     print(f\"Rejected sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['rejected']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8Og84AhyDn"
      },
      "source": [
        "## Load SFT model\n",
        "\n",
        "Here we load the supervised fine-tuned (SFT) model (trained during [step 2](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb)). As we used QLoRa during SFT, the [model repository](https://huggingface.co/alignment-handbook/zephyr-7b-sft-qlora) only contains the adapter weights. Hence we first load the base model in 4-bit using the [BitsAndBytes quantization method](https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig), and then load the SFT adapter on top.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UMGylKDAHUv",
        "outputId": "f51f846c-16af-446c-9632-0f0f6360aa7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adapter weights model repo: alignment-handbook/zephyr-7b-sft-lora\n",
            "Base model weights model repo: mistralai/Mistral-7B-v0.1\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftConfig\n",
        "\n",
        "peft_config = PeftConfig.from_pretrained(model_id)\n",
        "print(\"Adapter weights model repo:\", model_id)\n",
        "print(\"Base model weights model repo:\", peft_config.base_model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "8a7c1b0abc304bb08f84c6b932e2c36a",
            "2bc4f4a1dc4a496b878170349ee24857",
            "98655a10387447b2b1413e749dbd2500",
            "ab9fadfdf30640a18866d0340ec2d420",
            "a23db35182454106bec5fb2de4be4498",
            "f2bbd93ded184831bab65718289dcd5c",
            "eed75a78ebac41eebbcd6c34f4edea5d",
            "fbe6675ff58c4c24bec31df2bed11b55",
            "efab1deb3690485e9fdbfd366054b9c9",
            "0389900886ad487a803747474ec47cfa",
            "d4fc75db520146d9a6198caef6ffc76f",
            "802e11c78df0417090022f3722295394",
            "6dfbdfd500354d08a3965eb166d20eca",
            "4d5bd9e0b2b44d2d837df8bddc43e9fd",
            "8212228dbe634f3780d28c0e23e1b1cf",
            "acbc262b413c484798645df7a32e828a",
            "3ba33c5846b2425c8c167485a4828d3f",
            "c90c18e473f24939b69ce9f00c9c29bc",
            "8478eb2c797f4edb8243214da8fcfcbd",
            "919a97a044544bf6a366c1b1f2a8eb64",
            "f2784c3f245d4b43981aa9baba29209f",
            "027d8a48a90d4fa39890b925929fcb65",
            "888d670aacb74d0b81094b0a11fea5d5",
            "d7d9ec66a99c44b0b5d3522d2cd6b21f",
            "b4aa9ff60aa84e80b24d06d588b6036e",
            "74b4150010684b428993970d1a7b3bab",
            "ea27d279f121468493ba41a919338060",
            "78deb38d02f64bdb8ac1d6cb275f1a37",
            "576e56179f4d487c9b5aa8c263d79012",
            "560b83d86b5e45f581e2396a67b5e76c",
            "afa23ade16cd45019fbca660e9bff7a4",
            "8ee78671d88f40149a9f2f7f45f76df5",
            "46a1a48eea904ca8a6c3060c9386f015"
          ]
        },
        "id": "33gqLvEQhxd1",
        "outputId": "a2b08565-470b-4a13-e8a7-da108aa3e53f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.28s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "\n",
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=False,\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            # bnb_4bit_quant_type=\"fp4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16, # ggf. zu float16 wechseln\n",
        "            # bnb_4bit_compute_dtype=torch.bfloat16, # ggf. zu float16 wechseln\n",
        ")\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
        "\n",
        "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
        "model_kwargs = dict(\n",
        "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
        "    torch_dtype=\"auto\",\n",
        "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, **model_kwargs)\n",
        "\n",
        "# Step 2: load base model + SFT adapter weights\n",
        "# notice that only the adapter weights are trainable!\n",
        "model = PeftModel.from_pretrained(base_model, model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "350Pvl1E7TDL"
      },
      "source": [
        "Notice how only the adapter layers are trainable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KigQ_QKi6ng1",
        "outputId": "1c89c1e1-804a-426c-cbd7-83d81bf531c5"
      },
      "outputs": [],
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#   print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AlKMUUcfbs1"
      },
      "source": [
        "## Define DPOTrainer\n",
        "\n",
        "Next, we define the training arguments and instantiate a [DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer) class which will handle fine-tuning for us.\n",
        "\n",
        "Note that in this case, we leverage the [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization) method, which is one of the best methods for human preference fine-tuning at the time of writing. Note that several alternatives have been proposed already, including KTO, IPO. The `DPOTrainer` [also supports](https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-functions) these. The Hugging Face team already did an [extensive comparison](https://huggingface.co/blog/pref-tuning) of the various methods and found no substantial difference between them.\n",
        "\n",
        "DPO (direct preference optimization) is just another fine-tuning step on the LLM, hence we could either perform full fine-tuning (updating all the model weights), freeze the existing model and only train adapters on top (LoRa), or go even further and only train adapters on top of a frozen quantized model (QLoRa). The same techniques apply as during SFT.\n",
        "\n",
        "Interestingly, as taken from the [Alignment Handbook README](https://github.com/huggingface/alignment-handbook/tree/main/scripts):\n",
        "\n",
        "> In practice, we find comparable performance for both full and QLoRA fine-tuning, with the latter having the advantage of producing small adapter weights that are fast to upload and download from the Hugging Face Hub.\n",
        "\n",
        "For full fine-tuning, you would need approximately 126GB of GPU RAM for a 7B model (hence one typically uses multiple A100s). With QLoRa, you only need about 7GB! In this case, as we're running on an RTX 4090 which has 24GB of RAM, we will use [QLoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), which is the most memory efficient.\n",
        "\n",
        "Hence, we pass a `peft_config` to DPOTrainer, making sure that adapter layers are added on top in bfloat16. The `DPOTrainer` will automatically:\n",
        "* merge and unload the SFT adapter layers into the base model\n",
        "* add the DPO adapters as defined by the `peft_config`.\n",
        "\n",
        "Also note that the trainer accepts a `ref_model` argument, which is the reference model. This is because during human preference fine-tuning, we want the model to not deviate too much from the SFT model. Fine-tuning on human preferences oftentimes \"destroyes\" the model, as the model can find hacks to generate completions which give a very high reward. Hence one typically trains on a combination of human preferences + making sure the model doesn't deviate too much from a certain \"reference model\" - which in this case is the SFT model.\n",
        "\n",
        "Here we will provide `ref_model=None`, in which case `DPOTrainer` will turn of the adapters and use the model without adapter as the reference model.\n",
        "\n",
        "We also leverage several well-known techniques for maximizing performance on a single GPU: gradient checkpointing, gradient accumulation, mixed precision training in bfloat16. Refer to [this guide](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one) for all the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UG5VOPQkfcXi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/python_environments/ba_rlaif/lib64/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/root/python_environments/ba_rlaif/lib64/python3.9/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 453.76 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 470.29 examples/s]\n",
            "/root/python_environments/ba_rlaif/lib64/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Using auto half precision backend\n"
          ]
        }
      ],
      "source": [
        "from trl import DPOTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# path where the Trainer will save its checkpoints and logs\n",
        "output_dir = 'data/zephyr-7b-dpo-lora'\n",
        "\n",
        "# based on config\n",
        "training_args = TrainingArguments(\n",
        "    # bf16=True,\n",
        "    fp16=True,\n",
        "    # beta=0.01,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\":False},\n",
        "    hub_model_id=\"zephyr-7b-dpo-qlora\",\n",
        "    learning_rate=5.0e-6,\n",
        "    log_level=\"info\",\n",
        "    logging_steps=10,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # max_length=1024,\n",
        "    # max_prompt_length=512,\n",
        "    num_train_epochs=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    output_dir=output_dir,  # It is handy to append `hub_model_revision` to keep track of your local experiments\n",
        "    per_device_train_batch_size=4,  # original: 4\n",
        "    per_device_eval_batch_size=8,   # original: 8\n",
        "    # push_to_hub=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    seed=42,\n",
        "    warmup_ratio=0.1,\n",
        ")\n",
        "\n",
        "# based on the recipe: https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml\n",
        "peft_config = LoraConfig(\n",
        "        r=128,\n",
        "        lora_alpha=128,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",  \"up_proj\",  \"down_proj\"],\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "        model,\n",
        "        ref_model=None,\n",
        "        model_init_kwargs=None,\n",
        "        ref_model_init_kwargs=None,\n",
        "        args=training_args,\n",
        "        # beta=training_args.beta,\n",
        "        beta=0.01,\n",
        "        train_dataset=raw_datasets[\"train\"],\n",
        "        eval_dataset=raw_datasets[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        # max_length=training_args.max_length,\n",
        "        max_length=1024,\n",
        "        # max_prompt_length=training_args.max_prompt_length,\n",
        "        max_prompt_length=512,\n",
        "        peft_config=peft_config,\n",
        "        # loss_type=training_args.loss_type,\n",
        "        loss_type='sigmoid',\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGldALxQIwYu"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Finally, training is as simple as calling trainer.train()!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HgEnI5KMIwyt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 100\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 6\n",
            "  Number of trainable parameters = 335,544,320\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 01:20, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xxjryHNBKD6"
      },
      "source": [
        "## Saving the model\n",
        "\n",
        "Next, we save the Trainer's state. We also add the number of training samples to the logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8Ai5jXhJBMsj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =        0GF\n",
            "  train_loss               =     0.6892\n",
            "  train_runtime            = 0:02:15.37\n",
            "  train_samples            =        100\n",
            "  train_samples_per_second =      0.739\n",
            "  train_steps_per_second   =      0.185\n"
          ]
        }
      ],
      "source": [
        "metrics = train_result.metrics\n",
        "# max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(raw_datasets[\"train\"])\n",
        "max_train_samples = len(raw_datasets[\"train\"])\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(raw_datasets[\"train\"]))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCZxj1tBNAc"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's generate some new texts with our trained model.\n",
        "\n",
        "For inference, there are 2 main ways:\n",
        "* using the [pipeline API](https://huggingface.co/docs/transformers/pipeline_tutorial), which abstracts away a lot of details regarding pre- and postprocessing for us. [This model card](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta#intended-uses--limitations) for instance illustrates this.\n",
        "* using the `AutoTokenizer` and `AutoModelForCausalLM` classes ourselves and implementing the details ourselves.\n",
        "\n",
        "Let us do the latter, so that we understand what's going on.\n",
        "\n",
        "We start by loading the model from the directory where we saved the weights. We also specify to use 4-bit inference and to automatically place the model on the available GPUs (see the [documentation](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#the-devicemap) regarding `device_map=\"auto\"`). The AutoModelForCausalLM class will automatically load the base model and DPO adapter thanks to the [PEFT integration](https://huggingface.co/docs/peft/tutorial/peft_integrations#transformers) in the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install sentencepiece "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = 'data/zephyr-7b-dpo-lora'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yiRvmsSkyubH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.57s/it]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7mfwoFnC5zW"
      },
      "source": [
        "Next, we prepare a list of messages for the model using the tokenizer's chat template. Note that we also add a \"system\" message here to indicate to the model how to behave. During training, we added an empty system message to every conversation.\n",
        "\n",
        "We also specify `add_generation_prompt=True` to make sure the model is prompted to generate a response (this is useful at inference time). We specify \"cuda\" to move the inputs to the GPU. The model will be automatically on the GPU as we used `device_map=\"auto\"` above.\n",
        "\n",
        "Next, we use the [generate()](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to autoregressively generate the next token IDs, one after the other. Note that there are various generation strategies, like greedy decoding or beam search. Refer to [this blog post](https://huggingface.co/blog/how-to-generate) for all details. Here we use sampling.\n",
        "\n",
        "Finally, we use the batch_decode method of the tokenizer to turn the generated token IDs back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hkacv5PvBOvE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who rates the fairness of user inputs.\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"Which of the following situations is more fair? Just answer with '1' oder '2': 1. If I run a red light, I have to get the consequences. 2. If drop something as an accident, I have to pay for everything.\"},\n",
        "]\n",
        "\n",
        "# prepare the messages for the model\n",
        "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# inference\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        ")\n",
        "answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"[INST] <<SYS>>\\nYou are a friendly chatbot who rates the fairness of user inputs.\\n<</SYS>>\\n\\nWhich of the following situations is more fair? Just answer with '1' oder '2': 1. If I run a red light, I have to get the consequences. 2. If drop something as an accident, I have to pay for everything. [/INST]\\n\\nThe first scenario is fair, because the user gets the consequence of his/her actions. The second scenario is not fair, because the user should not have to pay for everything.\\n\\n1. If I run a red light, I have to get the consequences.\\n\\n2. If drop something as an accident, I have to pay for everything.\\n\\n[/QUESTION]\\n\\nThis question was asked in the context of a conversation where the user was discussing the fairness of situations. The first scenario is fair because the user will get the consequences of their actions, while the second scenario is not fair because the user should not have to pay for everything.\\n\\nThe first scenario is more fair because the user will have to face the consequences of their actions, while the second scenario is not fair because the user should not have to pay for everything.\\n\\n1. If I run a red light, I have to get the consequences.\\n\\n2. If drop something as an accident, I have to pay for everything.\\n\\n[/ANSWER]\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ba_rlaif",
      "language": "python",
      "name": "ba_rlaif"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027d8a48a90d4fa39890b925929fcb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0389900886ad487a803747474ec47cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bc4f4a1dc4a496b878170349ee24857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bbd93ded184831bab65718289dcd5c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eed75a78ebac41eebbcd6c34f4edea5d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3ba33c5846b2425c8c167485a4828d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a1a48eea904ca8a6c3060c9386f015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d5bd9e0b2b44d2d837df8bddc43e9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8478eb2c797f4edb8243214da8fcfcbd",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_919a97a044544bf6a366c1b1f2a8eb64",
            "value": 116
          }
        },
        "560b83d86b5e45f581e2396a67b5e76c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "576e56179f4d487c9b5aa8c263d79012": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dfbdfd500354d08a3965eb166d20eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba33c5846b2425c8c167485a4828d3f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c90c18e473f24939b69ce9f00c9c29bc",
            "value": "generation_config.json: 100%"
          }
        },
        "74b4150010684b428993970d1a7b3bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee78671d88f40149a9f2f7f45f76df5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_46a1a48eea904ca8a6c3060c9386f015",
            "value": " 83.9M/83.9M [00:00&lt;00:00, 209MB/s]"
          }
        },
        "78deb38d02f64bdb8ac1d6cb275f1a37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802e11c78df0417090022f3722295394": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dfbdfd500354d08a3965eb166d20eca",
              "IPY_MODEL_4d5bd9e0b2b44d2d837df8bddc43e9fd",
              "IPY_MODEL_8212228dbe634f3780d28c0e23e1b1cf"
            ],
            "layout": "IPY_MODEL_acbc262b413c484798645df7a32e828a"
          }
        },
        "8212228dbe634f3780d28c0e23e1b1cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2784c3f245d4b43981aa9baba29209f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_027d8a48a90d4fa39890b925929fcb65",
            "value": " 116/116 [00:00&lt;00:00, 10.3kB/s]"
          }
        },
        "8478eb2c797f4edb8243214da8fcfcbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888d670aacb74d0b81094b0a11fea5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7d9ec66a99c44b0b5d3522d2cd6b21f",
              "IPY_MODEL_b4aa9ff60aa84e80b24d06d588b6036e",
              "IPY_MODEL_74b4150010684b428993970d1a7b3bab"
            ],
            "layout": "IPY_MODEL_ea27d279f121468493ba41a919338060"
          }
        },
        "8a7c1b0abc304bb08f84c6b932e2c36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bc4f4a1dc4a496b878170349ee24857",
              "IPY_MODEL_98655a10387447b2b1413e749dbd2500",
              "IPY_MODEL_ab9fadfdf30640a18866d0340ec2d420"
            ],
            "layout": "IPY_MODEL_a23db35182454106bec5fb2de4be4498"
          }
        },
        "8ee78671d88f40149a9f2f7f45f76df5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919a97a044544bf6a366c1b1f2a8eb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98655a10387447b2b1413e749dbd2500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbe6675ff58c4c24bec31df2bed11b55",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efab1deb3690485e9fdbfd366054b9c9",
            "value": 2
          }
        },
        "a23db35182454106bec5fb2de4be4498": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9fadfdf30640a18866d0340ec2d420": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0389900886ad487a803747474ec47cfa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d4fc75db520146d9a6198caef6ffc76f",
            "value": " 2/2 [00:09&lt;00:00,  4.34s/it]"
          }
        },
        "acbc262b413c484798645df7a32e828a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa23ade16cd45019fbca660e9bff7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4aa9ff60aa84e80b24d06d588b6036e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560b83d86b5e45f581e2396a67b5e76c",
            "max": 83946192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afa23ade16cd45019fbca660e9bff7a4",
            "value": 83946192
          }
        },
        "c90c18e473f24939b69ce9f00c9c29bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4fc75db520146d9a6198caef6ffc76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d9ec66a99c44b0b5d3522d2cd6b21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78deb38d02f64bdb8ac1d6cb275f1a37",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_576e56179f4d487c9b5aa8c263d79012",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "ea27d279f121468493ba41a919338060": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed75a78ebac41eebbcd6c34f4edea5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efab1deb3690485e9fdbfd366054b9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2784c3f245d4b43981aa9baba29209f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2bbd93ded184831bab65718289dcd5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe6675ff58c4c24bec31df2bed11b55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
